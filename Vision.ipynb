{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "During the first half of this notebook, we'll walk through some basic image loading and pre-processing techniques in Python. Then, we'll plug into Microsoft Azure to play with some of their AI tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Images\n",
    "When a computer looks at an image, it doesn't see shapes and objects. Instead, it sees grids of pixels. AI models are able to process these numerical values to achieve some pretty amazing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an Image\n",
    "We'll start by loading a .JPG image file. Run the following cell of code to load and display the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "img_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\n",
    "\n",
    "# download the image and display it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what an image looks like to us as humans.\n",
    "\n",
    "### Examine the Image\n",
    "Now let's see how the computer sees the image. Run the cell below to convert the image to a **matrix** (a grid of numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the image is represented as a grid of pixel values. Keep in mind, since there are so many values our code doesn't print them all. As a human, this looks like nothing more than gibberish, however, computers are great at finding patterns in the raw pixels that can be used to detect similar objects. \n",
    "\n",
    "Next let's find the pixel dimensions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(height, width, color_channels) = img.shape\n",
    "print(\"height = \" + str(height))\n",
    "print(\"width = \" + str(width))\n",
    "print(\"color channels = \" + str(color_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the image is 416 pixels tall by 416 pixels wide. We also see that the image has 3 color channels, which is another way for saying we have 3 color values for each pixel: <span style=\"color: red;\">Red</span>, <span style=\"color: green;\">Green</span>, and <span style=\"color: blue;\">Blue</span>.\n",
    "\n",
    "Let's take a look at a pixel in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_x = 200\n",
    "pixel_y = 350\n",
    "pixel_values = img[pixel_y, pixel_x, :]\n",
    "pixel_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, we can see that the pixel at (200, 350) has a RGB color value of: (51, 92, 216)</p>\n",
    "<p>Earlier we discussed that in computer vision, every pixel value is a <b>feature</b>. Let's see how many features we have in our image.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = height * width * color_channels\n",
    "print(str(height) + \" x \" + str(width) + \" x \" + str(color_channels) + \" = \" + str(num_features) + \" total features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a <b>LOT</b> of features for our model to process! Let's see if we can use some pre-processing techniques to shrink that down a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Pre-Processing\n",
    "Today we learned that it's sometimes helpful to pre-process images before feeding them to our model. Pre-processing is a way to manipulate an image in a way that makes it easier for a model to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscaling\n",
    "One of the first (and most basic) pre-processing techniques we discussed was grayscaling. By removing color from an image, we significantly reduce the feature size of the image. Let's do this by selecting only the first color channel from out original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscaled = img[:,:,0]\n",
    "plt.imshow(grayscaled, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our image now has 1 color channel instead of 3. Now let's calculate the total number of features again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = height * width\n",
    "print(str(height) + \" x \" + str(width) + \" = \" + str(num_features) + \" total features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We were able to reduce our total feature size by 2/3! Keep in mind, however, that in some problems we will <i>need</i> to keep color information. For instance, if we were building an image classifier to detect dog breeds, it would be very difficult to tell the difference between a brown dog and a black dog from only a grayscale image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing\n",
    "Another pre-processing technique we talked about is normalization. Machine learning models tend to learn much easier if all of the input data is in the same range (usually 0-1).\n",
    "\n",
    "We know that pixel values range from 0-255. Let's print out the pixel values and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, each pixel value seems to be in the range of 0-255. In order to normalize these values, we can simply divide every pixel value by 255, which is the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_img = img / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's take a look at the values again to make sure they are in the range of 0-1.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our image is now properly normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping\n",
    "The final pre-processing step we discussed is cropping. A lot of times, models can become confused by unimportant background objects. An easy way to combat this is to properly crop only the perintent information in an object. For instance, if we were building a face recognition algorithm, we would want to crop just the face so that we are sure our model doesn't learn anything from the surrounding objects in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we can easily crop an image by specifying the coordinates of the desired region. Below we do this by specifying the min/max X values of the cropped image, as well as the min/max Y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = 150\n",
    "min_y = 100\n",
    "max_x = 270\n",
    "max_y = 210\n",
    "cropped = img[min_y:max_y, min_x:max_x, :]\n",
    "plt.imshow(img)\n",
    "plt.imshow(cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we had to manually select the exact coordinates of the crop, we will soon use tools from Microsoft that can automatically detect and crop faces for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "Sometimes we may want to train an image classifier but find ourselves unable to get our hands on a large image dataset. Luckily, we can use **image augmentation** to generate a bunch of different variations of our data. This provides the model with more training examples with different lighting conditions and orientations. \n",
    "\n",
    "Again, we'll load our original colored image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "img_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\n",
    "\n",
    "# download the image and display it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rotate the image 30 degrees to produce a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated = img.rotate(30)\n",
    "plt.imshow(rotated);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a new, brighter image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhancer = ImageEnhance.Brightness(img)\n",
    "bright = enhancer.enhance(3)\n",
    "plt.imshow(bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll flip the image horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = np.flip(img, 1)\n",
    "plt.imshow(flipped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining these methods, we can randomly generate a bunch of variations of an image. Let's test this out by generating some examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "columns = 4\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(1, rows*columns + 1):\n",
    "    orig_img = img.copy()\n",
    "    shape = np.array(orig_img).shape\n",
    "    \n",
    "     # randomly brighten/darken the image\n",
    "    brightness = np.random.uniform(0.1, 5, size=1)[0]\n",
    "    enhancer = ImageEnhance.Brightness(orig_img)\n",
    "    orig_img = enhancer.enhance(brightness)\n",
    "    \n",
    "    # randomly rotate the image\n",
    "    rotation = np.random.randint(45, size=1)[0]\n",
    "    orig_img = orig_img.rotate(rotation)\n",
    "    \n",
    "    # randomly flip the image\n",
    "    shouldFlip = np.random.randint(2, size=1)[0]\n",
    "    if shouldFlip:\n",
    "        orig_img = np.flip(orig_img, 1)\n",
    "        \n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(orig_img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now seen how we can randomly transform an image to expand our dataset. There are many other transformations we could use to have an even more diverse dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microsoft Azure's Cognitive Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ Create a resource**, and search **Computer Vision**.\n",
    "4. Click on **Computer Vision** and in the **Create** blade, enter the following details, and then click **Review and Create** and then **Create**\n",
    "  * **Subscription**: Azure for Students or Microsoft Azure Sponsorship\n",
    "  * **Resource Group**: Choose the existing resource group you created in the previous lab. If you didn't complete the previous lab, click on **Create new**, enter a unique name, and click **Ok** to create a new resource group.)\n",
    "  * **Region**: You may use: Central US, East US, West US, West US2, West US3, South Central US, West Central US\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Pricing tier**: Choose the F0 pricing tier if on an Azure for Students subscription or the Standard S1 pricing tier if on a Microsoft Azure Sponsorship.\n",
    "5. Wait for the service to be created and when the deployment is complete, click **Go to resource**.\n",
    "6. In your resource's blade, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **visionKey** variable assignment value in the cell below. \n",
    "7. Run the cell below to assign the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visionKey = 'YOUR_KEY' #p9234kjk;jf;sldf92q34!@@98-//!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an Image\n",
    "Let's start with the same image we analyzed previously.\n",
    "\n",
    "Run the code in the cell below to retrieve the original colored image from the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    visionKey\n",
    "    import matplotlib.pyplot as plt\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    from io import BytesIO\n",
    "    %matplotlib inline\n",
    "\n",
    "    img_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\n",
    "\n",
    "    # download the image and display it\n",
    "    response = requests.get(img_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    plt.imshow(img);\n",
    "except:\n",
    "    print(\"Make sure you run the code cell above that assigns the visionKey variable! Then run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Computer Vision API to Get Image Features\n",
    "The Computer Vision API uses a machine learning model that has been pre-trained on millions of images.\n",
    "\n",
    "Run the cell below to see what caption the Computer Vision API suggests for the image above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Uncomment, meaning delete the # sign in order to select the region you\n",
    "set up for your resource! If you do not do this, it will not run!\n",
    "\n",
    "Exa: If your resource runs in \"Central US\", you must keep all the other visionURI\n",
    "with the #, but delete the one with \"CentralUS\" in the name. Remember, keep the \n",
    "other ones with the # sign!\n",
    "\n",
    "visionURI = 'CentralUS.api.cognitive.microsoft.com'\n",
    "\n",
    "\"\"\"\n",
    "#visionURI = 'CentralUS.api.cognitive.microsoft.com'\n",
    "#visionURI = 'EastUS.api.cognitive.microsoft.com'\n",
    "#visionURI = 'WestUS.api.cognitive.microsoft.com'\n",
    "#visionURI = 'WestUS2.api.cognitive.microsoft.com'\n",
    "#visionURI = 'WestUS3.api.cognitive.microsoft.com'\n",
    "#visionURI = 'SouthCentralUS.api.cognitive.microsoft.com'\n",
    "#visionURI = 'WestCentralUS.api.cognitive.microsoft.com'\n",
    "\n",
    "\n",
    "def get_image_features(img_url):\n",
    "    import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "    headers = {\n",
    "        # Request headers.\n",
    "        'Content-Type': 'application/json',\n",
    "        'Ocp-Apim-Subscription-Key': visionKey,\n",
    "    }\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        # Request parameters. All of them are optional.\n",
    "        'visualFeatures': 'Categories,Description,Color',\n",
    "        'language': 'en',\n",
    "    })\n",
    "\n",
    "    body = \"{'url':'\" + img_url + \"'}\"\n",
    "\n",
    "    try:\n",
    "        # Execute the REST API call and get the response.\n",
    "        conn = http.client.HTTPSConnection(visionURI)\n",
    "        conn.request(\"POST\", \"/vision/v1.0/analyze?%s\" % params, body, headers)\n",
    "        response = conn.getresponse()\n",
    "        data = response.read()\n",
    "\n",
    "        # 'data' contains the JSON response.\n",
    "        parsed = json.loads(data.decode(\"UTF-8\"))\n",
    "        if response is not None:\n",
    "            return parsed\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:')\n",
    "        print(e)\n",
    "        \n",
    "jsonData = get_image_features(img_url)\n",
    "desc = jsonData['description']['captions'][0]['text']\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description is pretty spot on. This is not a trick! Azure's model has been trained on many images from the web - including pictures of Mark Cuban.  AI is really that good!\n",
    "\n",
    "Run the cell below to see the full JSON response, including image properties and suggested tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# View the full details returned\n",
    "print (json.dumps(jsonData, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a different image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image and show it\n",
    "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/uke.jpg'\n",
    "\n",
    "# download the image and display it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "jsonData = get_image_features(img_url)\n",
    "desc = jsonData['description']['captions'][0]['text']\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about something a little more complex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image and show it\n",
    "img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/soccer.jpg'\n",
    "\n",
    "# download the image and display it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "jsonData = get_image_features(img_url)\n",
    "desc = jsonData['description']['captions'][0]['text']\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Face API\n",
    "While the Computer Vision API is useful for general image analysis, the Face API offers specific functions for analyzing faces in images. This can be useful in a variety of AI scenarios.\n",
    "\n",
    "### Create a Face API Service\n",
    "To provision a Computer Vision API service in your Azure subscription, Follow these steps:\n",
    "\n",
    "1. Open another browser tab and navigate to https://portal.azure.com.\n",
    "2. Sign in using your Microsoft account.\n",
    "3. Click **+ Create a Resource**, and search **Face**.\n",
    "4. Click **Face** and in the **Create** blade, enter the following details, and then click **Review and Create** and then **Create**\n",
    "  * **Subscription**: Azure for Students or Microsoft Azure Sponsorship\n",
    "  * **Resource Group**: Choose the existing resource group you created earlier.\n",
    "  * **Region**: You must use the same region as previously selected: Central US, East US, West US, West US2, West US3, South Central US, West Central US\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Pricing tier**: Choose the F0 pricing tier if on an Azure for Students subscription or the S0 pricing tier if on a Microsoft Azure Sponsorship.\n",
    "  \n",
    "Also, be sure to check the box verifying this service is not being used by or for a police department in the US.\n",
    "5. Wait for the service to be created.\n",
    "6. When deployment is complete, click **Go to Resource**.\n",
    "7. In your resource's blade, click **Keys and Endpoint** and then copy **Key 1** to the clipboard and paste it into the **faceKey** variable assignment value in the cell below.\n",
    "8. Run the cell below to assign the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceKey = \"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Face API has a Python SDK, which you can install as a package. This makes it easier to work with.\n",
    "\n",
    "Run the following cell to install the Face SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    faceKey\n",
    "except:\n",
    "    print(\"Make sure you run the code cell above that assigns the faceKey variable! Then run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to use the Face API. First, let's see if we can detect a face in an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cognitive_face as CF\n",
    "\n",
    "\"\"\"\n",
    "Uncomment, meaning delete the # sign in order to select the region you\n",
    "set up for your resource! If you do not do this, it will not run!\n",
    "\n",
    "Exa: If your resource runs in \"Central US\", you must keep all the other visionURI\n",
    "with the #, but delete the one with \"CentralUS\" in the name. Remember, keep the \n",
    "other ones with the # sign!\n",
    "\n",
    "faceURI = 'CentralUS.api.cognitive.microsoft.com'\n",
    "\n",
    "\"\"\"\n",
    "#faceURI = 'CentralUS.api.cognitive.microsoft.com'\n",
    "#faceURI = 'EastUS.api.cognitive.microsoft.com'\n",
    "#faceURI = 'WestUS.api.cognitive.microsoft.com'\n",
    "#faceURI = 'WestUS2.api.cognitive.microsoft.com'\n",
    "#faceURI = 'WestUS3.api.cognitive.microsoft.com'\n",
    "#faceURI = 'SouthCentralUS.api.cognitive.microsoft.com'\n",
    "#faceURI = 'WestCentralUS.api.cognitive.microsoft.com'\n",
    "\n",
    "# Set URI and Key\n",
    "CF.BaseUrl.set(faceURI)\n",
    "CF.Key.set(faceKey)\n",
    "\n",
    "\n",
    "# Detect faces in an image\n",
    "img_url = 'https://thumbor.forbes.com/thumbor/fit-in/416x416/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5d8acdb96de3150009a5060c%2F0x0.jpg%3Fbackground%3D000000%26cropX1%3D1338%26cropX2%3D3471%26cropY1%3D239%26cropY2%3D2372'\n",
    "result = CF.face.detect(img_url)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Face API has detected one face, and assigned it an ID. It also returns the coordinates for the top left corner and the width and height for the rectangle within which the face is detected.\n",
    "\n",
    "Run the cell below to show the rectange on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Get the image\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Add rectangles for each face found\n",
    "color=\"red\"\n",
    "if result is not None:\n",
    "    draw = ImageDraw.Draw(img) \n",
    "    for currFace in result:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        left = faceRectangle['left']\n",
    "        top = faceRectangle['top']\n",
    "        width = faceRectangle['width']\n",
    "        height = faceRectangle['height']\n",
    "        draw.line([(left,top),(left+width,top)],fill=color, width=5)\n",
    "        draw.line([(left+width,top),(left+width,top+height)],fill=color , width=5)\n",
    "        draw.line([(left+width,top+height),(left, top+height)],fill=color , width=5)\n",
    "        draw.line([(left,top+height),(left, top)],fill=color , width=5)\n",
    "\n",
    "# show the image\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to detecting the face, the Face API also assigned an ID to this face. The ID is saved by the service for a while, enabling you to reference it. Run the following cell to see the ID assigned to the face that has been detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face1 = result[0]['faceId']\n",
    "print (\"Face 1:\" + face1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful thing you can do with the face ID is to compare images and see if a matching face is found. This kind of facial comparison is common in a variety of security / user authentication scenarios.\n",
    "\n",
    "Let's try it with another image of the same person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image to compare\n",
    "img2_url = 'https://markcubancompanies.com/wp-content/uploads/2020/05/Cuban-1-e1589823008367.jpeg'\n",
    "response2 = requests.get(img2_url)\n",
    "img2 = Image.open(BytesIO(response2.content))\n",
    "\n",
    "# Detect faces in a comparison image\n",
    "result2 = CF.face.detect(img2_url)\n",
    "\n",
    "# Assume the first face is the one we want to compare\n",
    "if result2 is not None:\n",
    "    face2 = result2[0]['faceId']\n",
    "    print (\"Face 2:\" + face2)\n",
    "\n",
    "def verify_face(face1, face2):\n",
    "    # By default, assume the match is unverified\n",
    "    verified = \"Not Verified\"\n",
    "    color=\"red\"\n",
    "\n",
    "    # compare the comparison face to the original one we retrieved previously\n",
    "    verify = CF.face.verify(face1, face2)\n",
    "\n",
    "    # if there's a match, set verified and change color to green\n",
    "    if verify['isIdentical'] == True:\n",
    "        verified = \"Verified\"\n",
    "        color=\"lightgreen\"\n",
    "\n",
    "    # Display the second face with a red rectange if unverified, or green if verified\n",
    "    draw = ImageDraw.Draw(img2) \n",
    "    for currFace in result2:\n",
    "        faceRectangle = currFace['faceRectangle']\n",
    "        left = faceRectangle['left']\n",
    "        top = faceRectangle['top']\n",
    "        width = faceRectangle['width']\n",
    "        height = faceRectangle['height']\n",
    "        draw.line([(left,top),(left+width,top)] , fill=color, width=5)\n",
    "        draw.line([(left+width,top),(left+width,top+height)] , fill=color, width=5)\n",
    "        draw.line([(left+width,top+height),(left, top+height)] , fill=color, width=5)\n",
    "        draw.line([(left,top+height),(left, top)] , fill=color, width=5)\n",
    "\n",
    "    # show the image\n",
    "    imshow(img2)\n",
    "\n",
    "    # Display verification status and confidence level\n",
    "    print(verified)\n",
    "    print (\"Confidence Level: \" + str(verify['confidence']))\n",
    "\n",
    "verify_face(face1, face2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try to match the original face to a different person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image to compare\n",
    "img2_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/satya.jpg'\n",
    "response2 = requests.get(img2_url)\n",
    "img2 = Image.open(BytesIO(response2.content))\n",
    "\n",
    "# Detect faces in a comparison image\n",
    "result2 = CF.face.detect(img2_url)\n",
    "\n",
    "# Assume the first face is the one we want to compare\n",
    "face2 = result2[0]['faceId']\n",
    "print (\"Face 2:\" + face2)\n",
    "\n",
    "verify_face(face1, face2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No match!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Custom Vision Cognitive Service\n",
    "The *Custom Vision* cognitive service enables you to create custom computer vision solutions.\n",
    "\n",
    "In this notebook, you will create and train a Custom Vision *image classification* project that can identify pictures of apples and carrots, and use it to classify new images.\n",
    "\n",
    "&gt; **Note**: *Some of the images used in the lab are sourced from the free image library at <a>www.pachd.com</a>*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and extract the images you will use to train your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/produce.zip -o produce.zip\n",
    "!unzip produce.zip",
    "!pip install azure-cognitiveservices-vision-customvision","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Custom Vision service instance\n",
    "Now you're ready to use the Custom Vision service. You'll need to create an instance of the service and get your unique training and prediction keys so you can access it:\n",
    "1. Go to https://customvision.ai/ and sign in using the Microsoft account associated with your Azure subscription.\n",
    "2. Click the *Settings* (⚙) icon at the top right and click **create new** to create a new resource.\n",
    "3. Enter the following information and click **Create resource**:\n",
    "  * **Name**: A unique name for your service.\n",
    "  * **Subscription**: Azure for Students or Microsoft Azure Sponsorship\n",
    "  * **Resource Group**: Choose the existing resource group you created earlier.\n",
    "  * **Kind**: CognitiveServices\n",
    "  * **Location**: You may use: Central US, East US, West US, West US2, West US3, South Central US, West Central US. When running in Binder, it does not have to be the same as the locations for Face API and Computer Vision API that you just used.\n",
    "  * **Pricing tier**: Choose the F0 or S0 pricing tier.\n",
    "4. Wait for the resource to be created.\n",
    "5. Copy *Key* to **customVisionKey**, *Endpoint* to **endpoint**, and *Resource Id* to **resourceID* in variables below and **run the cell**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customVisionKey = 'YOUR_KEY'\n",
    "endpoint = 'YOUR_ENDPOINT'\n",
    "resourceID = 'YOUR_RESOURCE_ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Custom Vision project\n",
    "Now we'll create a project for the apple/carrot classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    customVisionKey\n",
    "    endpoint\n",
    "    resourceID\n",
    "    \n",
    "    from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "    from msrest.authentication import ApiKeyCredentials\n",
    "\n",
    "    credentials = ApiKeyCredentials(in_headers={"Training-key": customVisionKey})\n",
    "    trainer = CustomVisionTrainingClient(credentials, endpoint)\n",
    "\n",
    "    # Create a new project\n",
    "    print (\"Creating project...\")\n",
    "    project = trainer.create_project(\"Produce Classification\")\n",
    "    print(\"The project was created!\")\n",
    "except:\n",
    "    print(\"Make sure you run the code cell above that assigns the customVisionKey, endpoint, and resourceID variables! Then run this cell again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add tags\n",
    "The project will identify images as apples or carrots, so we'll need tags for those classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two tags in the new project\n",
    "print(\"Creating tags...\")\n",
    "apple_tag = trainer.create_tag(project.id, \"Apple\")\n",
    "carrot_tag = trainer.create_tag(project.id, \"Carrot\")\n",
    "print('Created tags!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload training images\n",
    "Now that we've got the tags, we need to upload some images of apples and carrots, assign the appropriate tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Adding images...\")\n",
    "\n",
    "apples_dir = \"apples\"\n",
    "for image in os.listdir(apples_dir):\n",
    "    with open(os.path.join(apples_dir, image), mode=\"rb\") as img_data: \n",
    "        trainer.create_images_from_data(project.id, img_data.read(), [apple_tag.id])\n",
    "\n",
    "carrots_dir = \"carrots\"\n",
    "for image in os.listdir(carrots_dir):\n",
    "    with open(os.path.join(carrots_dir, image), mode=\"rb\") as img_data: \n",
    "        trainer.create_images_from_data(project.id, img_data.read(), [carrot_tag.id])\n",
    "        \n",
    "print('Added images!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to your Custom Vision service and click the *Home* (⌂) icon to return to the home page, and then open the ***Produce Classification*** project to view the images that have been uploaded and tagged.\n",
    "\n",
    "### Train the project\n",
    "With the tagged images in place, we're now ready to train a classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print (\"Training...\")\n",
    "# Train the project, checking status every 1 second\n",
    "iteration = trainer.train_project(project.id)\n",
    "while (iteration.status == \"Training\"):\n",
    "    iteration = trainer.get_iteration(project.id, iteration.id)\n",
    "    print (\"Training status: \" + iteration.status)\n",
    "    time.sleep(1)\n",
    "\n",
    "# The iteration is now trained. Publish it to the project endpoint\n",
    "trainer.publish_iteration(project.id, iteration.id, \"First Iteration\", resourceID)\n",
    "\n",
    "# Make it the default iteration\n",
    "iteration = trainer.update_iteration(project_id= project.id, iteration_id=iteration.id, name= \"First Iteration\", is_default=True)\n",
    "\n",
    "print (\"Trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the project to classify images\n",
    "Now that we have a trained project, we can use it to predict the class of new images that weren't in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "%matplotlib inline\n",
    "\n",
    "# Use two test images\n",
    "test_img1_url = 'http://www.pachd.com/free-images/food-images/apple-01.jpg'\n",
    "test_img2_url = 'http://www.pachd.com/free-images/food-images/carrot-02.jpg'\n",
    "\n",
    "test_image_urls = []\n",
    "test_image_urls.append(test_img1_url)\n",
    "test_image_urls.append(test_img2_url)\n",
    "\n",
    "# Create an instance of the prediction service\n",
    "predictor = CustomVisionPredictionClient(customVisionKey, endpoint=endpoint)\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Get the images and show the predicted classes\n",
    "for url_idx in range(len(test_image_urls)):\n",
    "    response = requests.get(test_image_urls[url_idx])\n",
    "    image_contents = Image.open(BytesIO(response.content))\n",
    "    results = predictor.classify_image_url(project_id=project.id, published_name=iteration.name, url=test_image_urls[url_idx])\n",
    "    # The results include a prediction for each tag, in descending order of probability - so we'll get the first one\n",
    "    prediction = results.predictions[0].tag_name + \": {0:.2f}%\".format(results.predictions[0].probability * 100)\n",
    "    # Subplot for image and its predicted class\n",
    "    a=fig.add_subplot(1,2,url_idx+1)\n",
    "    imgplot = plt.imshow(image_contents)\n",
    "    a.set_title(prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to go a step further? Try testing your model on new images from Google! To do this, replace **test_img1_url** and **test_img2_url** in the code above with the URLs of the images you would like to test. Then re-run the code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nice job!\n",
    "You have now learned how to use Microsoft Azure's computer vision tools to process images. In the next lab, you'll see how you can create a Custom Vision image classifier to recognize traffic signs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
